{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Natalia409634/Machine-Learning/blob/main/T4_LinealesHiperparametos.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Búsqueda de hiperparámetros en regresión lineal\n",
        "\n",
        "En este tutorial trabajaremos con los modelos de regresión lineal. En concreto construiremos modelos lineales aproximados por mínimos cuadrados, así como sus versiones en las que se incorporan las regularizaciones Ridge, Lasso y Elastic Net.\n",
        "\n",
        "Como último veremos una regresión polinómica"
      ],
      "metadata": {
        "id": "s_jEsE8W6HbZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tratamiento de datos\n",
        "# ==============================================================================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Gráficos\n",
        "# ==============================================================================\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import style\n",
        "import seaborn as sns\n",
        "\n",
        "# Preprocesado y modelado\n",
        "# ==============================================================================\n",
        "from scipy.stats import pearsonr\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.linear_model import RidgeCV\n",
        "from sklearn.linear_model import LassoCV\n",
        "from sklearn.linear_model import ElasticNetCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn import metrics\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.compose import make_column_selector as selector\n",
        "\n",
        "\n",
        "# Configuración matplotlib\n",
        "# ==============================================================================\n",
        "plt.rcParams['image.cmap'] = \"bwr\"\n",
        "plt.rcParams['savefig.bbox'] = \"tight\"\n",
        "style.use('ggplot') or plt.style.use('ggplot')\n",
        "\n",
        "\n",
        "# Configuración warnings\n",
        "# ==============================================================================\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "8W2YIMIi7SyF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rmse(y_test, y_test_pred):\n",
        "  \"\"\" Este es mi cálculo del error cuadrático medio \"\"\"\n",
        "  return np.sqrt(metrics.mean_squared_error(y_test, y_test_pred))"
      ],
      "metadata": {
        "id": "skKrA10C-h_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cargamos los datos"
      ],
      "metadata": {
        "id": "T5bni26R7hn3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "housing = fetch_california_housing()\n",
        "\n",
        "X = housing.data\n",
        "y = housing.target"
      ],
      "metadata": {
        "id": "ZL6u47Le7jNh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(housing.DESCR)"
      ],
      "metadata": {
        "id": "hJQzFXQErj1V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Holdout para la evaluación del modelo. 33% de los datos disponibles para test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.33, random_state=42)"
      ],
      "metadata": {
        "id": "j7PQK5_08EYF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pintaResultados (reg, n):\n",
        "  plt.subplots(figsize=(30, 5))\n",
        "  x = np.arange(y_test[:n].size)\n",
        "  pred = reg.predict(X=X_test)\n",
        "  plt.plot(x, y_test[:n], 'b.', x, pred[:n], 'g^')"
      ],
      "metadata": {
        "id": "N_n7pkh3XwQX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regresión lineal por mínimos cuadrados"
      ],
      "metadata": {
        "id": "PpkoVv3l-GjJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "reg_lin = LinearRegression()\n",
        "\n",
        "pipe_regr = Pipeline([\n",
        "    ('scale', scaler),\n",
        "    ('REGL', reg_lin)])\n",
        "\n",
        "np.random.seed(42)\n",
        "pipe_regr.fit(X=X_train, y=y_train)\n",
        "print(f\"RMSE de regresión lineal: {rmse(y_test, pipe_regr.predict(X=X_test))}\")\n",
        "pintaResultados(pipe_regr,50)"
      ],
      "metadata": {
        "id": "XeXx3Bsf-LAF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Coeficientes del modelo\n",
        "# ==============================================================================\n",
        "df_coeficientes = pd.DataFrame(\n",
        "                        {'predictor': housing.feature_names,\n",
        "                         'coef': pipe_regr['REGL'].coef_.flatten()}\n",
        "                  )\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(11, 3.84))\n",
        "ax.stem(df_coeficientes.predictor, df_coeficientes.coef, markerfmt=' ')\n",
        "plt.xticks(rotation=90, ha='right', size=5)\n",
        "ax.set_xlabel('variable')\n",
        "ax.set_ylabel('coeficientes')\n",
        "ax.set_title('Coeficientes del modelo');"
      ],
      "metadata": {
        "id": "896lpH-b_XlG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ridge\n"
      ],
      "metadata": {
        "id": "2rWfPbouDS3k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creación y entrenamiento del modelo (con búsqueda por CV del valor óptimo alpha)\n",
        "# ==============================================================================\n",
        "# Por defecto RidgeCV utiliza el mean squared error\n",
        "regrRidge = RidgeCV(\n",
        "            alphas          = np.logspace(-3, 8, 200),\n",
        "            fit_intercept   = True,\n",
        "            store_cv_values = True\n",
        "         )\n",
        "\n",
        "np.random.seed(42)\n",
        "_ = regrRidge.fit(X = X_train, y = y_train)\n",
        "print(f\"RMSE de regresión lineal - Ridge: {rmse(y_test, regrRidge.predict(X=X_test))}\")\n",
        "pintaResultados(regrRidge,50)"
      ],
      "metadata": {
        "id": "beTgTzLGDVfr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evolución de los coeficientes en función de alpha\n",
        "# ==============================================================================\n",
        "alphas = regrRidge.alphas\n",
        "coefs = []\n",
        "\n",
        "for alpha in alphas:\n",
        "    modelo_temp = Ridge(alpha=alpha, fit_intercept=False)\n",
        "    modelo_temp.fit(X_train, y_train)\n",
        "    coefs.append(modelo_temp.coef_.flatten())\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(7, 3.84))\n",
        "ax.plot(alphas, coefs)\n",
        "ax.set_xscale('log')\n",
        "ax.set_xlabel('alpha')\n",
        "ax.set_ylabel('coeficientes')\n",
        "ax.set_title('Coeficientes del modelo en función de la regularización');\n",
        "plt.axis('tight')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MWlY0HMHFIBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como vemos, a medida que aumenta el valor de alpha, la regularización es mayor y el valor de los coeficientes se reduce"
      ],
      "metadata": {
        "id": "kl8787AIF0_h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evolución del error en función de alpha\n",
        "# ==============================================================================\n",
        "# modelo.cv_values almacena el mse de cv para cada valor de alpha. Tiene\n",
        "# dimensiones (n_samples, n_targets, n_alphas)\n",
        "mse_cv = regrRidge.cv_values_.reshape((-1, 200)).mean(axis=0)\n",
        "mse_sd = regrRidge.cv_values_.reshape((-1, 200)).std(axis=0)\n",
        "\n",
        "# Se aplica la raíz cuadrada para pasar de mse a rmse\n",
        "rmse_cv = np.sqrt(mse_cv)\n",
        "rmse_sd = np.sqrt(mse_sd)\n",
        "\n",
        "# Se identifica el óptimo y el óptimo + 1std\n",
        "min_rmse     = np.min(rmse_cv)\n",
        "sd_min_rmse  = rmse_sd[np.argmin(rmse_cv)]\n",
        "min_rsme_1sd = np.max(rmse_cv[rmse_cv <= min_rmse + sd_min_rmse])\n",
        "optimo       = regrRidge.alphas[np.argmin(rmse_cv)]\n",
        "optimo_1sd   = regrRidge.alphas[rmse_cv == min_rsme_1sd]\n",
        "\n",
        "\n",
        "# Gráfico del error +- 1 desviación estándar\n",
        "fig, ax = plt.subplots(figsize=(7, 3.84))\n",
        "ax.plot(regrRidge.alphas, rmse_cv)\n",
        "ax.fill_between(\n",
        "    regrRidge.alphas,\n",
        "    rmse_cv + rmse_sd,\n",
        "    rmse_cv - rmse_sd,\n",
        "    alpha=0.2\n",
        ")\n",
        "\n",
        "ax.axvline(\n",
        "    x         = optimo,\n",
        "    c         = \"gray\",\n",
        "    linestyle = '--',\n",
        "    label     = 'óptimo'\n",
        ")\n",
        "\n",
        "ax.axvline(\n",
        "    x         = optimo_1sd,\n",
        "    c         = \"blue\",\n",
        "    linestyle = '--',\n",
        "    label     = 'óptimo_1sd'\n",
        ")\n",
        "ax.set_xscale('log')\n",
        "ax.set_ylim([0,None])\n",
        "ax.set_title('Evolución del error CV en función de la regularización')\n",
        "ax.set_xlabel('alpha')\n",
        "ax.set_ylabel('RMSE')\n",
        "plt.legend();"
      ],
      "metadata": {
        "id": "wYk2ZtPJF7Q5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mejor valor alpha encontrado\n",
        "# ==============================================================================\n",
        "print(f\"Mejor valor de alpha encontrado: {regrRidge.alpha_}\")"
      ],
      "metadata": {
        "id": "5TBg1A5kGne1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Coeficientes del modelo\n",
        "# ==============================================================================\n",
        "df_coeficientes = pd.DataFrame(\n",
        "                        {'predictor': housing.feature_names,\n",
        "                         'coef': regrRidge.coef_.flatten()}\n",
        "                  )\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(11, 3.84))\n",
        "ax.stem(df_coeficientes.predictor, df_coeficientes.coef, markerfmt=' ')\n",
        "plt.xticks(rotation=90, ha='right', size=5)\n",
        "ax.set_xlabel('variable')\n",
        "ax.set_ylabel('coeficientes')\n",
        "ax.set_title('Coeficientes del modelo');"
      ],
      "metadata": {
        "id": "MBrNgz36GyXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "En comparación al modelo por mínimos cuadrados ordinarios, con ridge, el orden de magnitud de los coeficientes es menor."
      ],
      "metadata": {
        "id": "VFshaxFfHFRr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lasso"
      ],
      "metadata": {
        "id": "BFzwibQtP6UJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creación y entrenamiento del modelo (con búsqueda por CV del valor óptimo alpha)\n",
        "# ==============================================================================\n",
        "# Por defecto LassoCV utiliza el mean squared error\n",
        "regrLasso = LassoCV(\n",
        "            alphas          = np.logspace(-9, 3, 200),\n",
        "            cv              = 10\n",
        "         )\n",
        "\n",
        "np.random.seed(42)\n",
        "_ = regrLasso.fit(X = X_train, y = y_train)\n",
        "print(f\"RMSE de regresión lineal: {rmse(y_test, regrLasso.predict(X=X_test))}\")\n",
        "pintaResultados(regrLasso,50)"
      ],
      "metadata": {
        "id": "xn6jNFCPP8hL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evolución de los coeficientes en función de alpha\n",
        "# ==============================================================================\n",
        "alphas = regrLasso.alphas_\n",
        "coefs = []\n",
        "\n",
        "for alpha in alphas:\n",
        "    modelo_temp = Lasso(alpha=alpha, fit_intercept=False)\n",
        "    modelo_temp.fit(X_train, y_train)\n",
        "    coefs.append(modelo_temp.coef_.flatten())\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(7, 3.84))\n",
        "ax.plot(alphas, coefs)\n",
        "ax.set_xscale('log')\n",
        "ax.set_ylim([-1,None])\n",
        "ax.set_xlabel('alpha')\n",
        "ax.set_ylabel('coeficientes')\n",
        "ax.set_title('Coeficientes del modelo en función de la regularización');"
      ],
      "metadata": {
        "id": "HW_CR-3lQ_dm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Puede ver se como, a medida que aumenta el valor de alpha, la regularización es mayor y más predictores quedan excluidos (su coeficiente es 0)."
      ],
      "metadata": {
        "id": "VGmK6pp-ROa4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Número de predictores incluidos (coeficiente !=0) en función de alpha\n",
        "# ==============================================================================\n",
        "alphas = regrLasso.alphas_\n",
        "n_predictores = []\n",
        "\n",
        "for alpha in alphas:\n",
        "    modelo_temp = Lasso(alpha=alpha, fit_intercept=False)\n",
        "    modelo_temp.fit(X_train, y_train)\n",
        "    coef_no_cero = np.sum(modelo_temp.coef_.flatten() != 0)\n",
        "    n_predictores.append(coef_no_cero)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(7, 3.84))\n",
        "ax.plot(alphas, n_predictores)\n",
        "ax.set_xscale('log')\n",
        "ax.set_ylim([-1,None])\n",
        "ax.set_xlabel('alpha')\n",
        "ax.set_ylabel('nº predictores')\n",
        "ax.set_title('Predictores incluidos en función de la regularización');"
      ],
      "metadata": {
        "id": "yu05PlMURODL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evolución del error en función de alpha\n",
        "# ==============================================================================\n",
        "# regrLasso.mse_path_ almacena el mse de cv para cada valor de alpha. Tiene\n",
        "# dimensiones (n_alphas, n_folds)\n",
        "mse_cv = regrLasso.mse_path_.mean(axis=1)\n",
        "mse_sd = regrLasso.mse_path_.std(axis=1)\n",
        "\n",
        "# Se aplica la raíz cuadrada para pasar de mse a rmse\n",
        "rmse_cv = np.sqrt(mse_cv)\n",
        "rmse_sd = np.sqrt(mse_sd)\n",
        "\n",
        "# Se identifica el óptimo y el óptimo + 1std\n",
        "min_rmse     = np.min(rmse_cv)\n",
        "sd_min_rmse  = rmse_sd[np.argmin(rmse_cv)]\n",
        "min_rsme_1sd = np.max(rmse_cv[rmse_cv <= min_rmse + sd_min_rmse])\n",
        "optimo       = regrLasso.alphas_[np.argmin(rmse_cv)]\n",
        "optimo_1sd   = regrLasso.alphas_[rmse_cv == min_rsme_1sd]\n",
        "\n",
        "# Gráfico del error +- 1 desviación estándar\n",
        "fig, ax = plt.subplots(figsize=(7, 3.84))\n",
        "ax.plot(regrLasso.alphas_, rmse_cv)\n",
        "ax.fill_between(\n",
        "    regrLasso.alphas_,\n",
        "    rmse_cv + rmse_sd,\n",
        "    rmse_cv - rmse_sd,\n",
        "    alpha=0.2\n",
        ")\n",
        "\n",
        "ax.axvline(\n",
        "    x         = optimo,\n",
        "    c         = \"gray\",\n",
        "    linestyle = '--',\n",
        "    label     = 'óptimo'\n",
        ")\n",
        "\n",
        "ax.axvline(\n",
        "    x         = optimo_1sd,\n",
        "    c         = \"blue\",\n",
        "    linestyle = '--',\n",
        "    label     = 'óptimo_1sd'\n",
        ")\n",
        "\n",
        "ax.set_xscale('log')\n",
        "ax.set_ylim([0,None])\n",
        "ax.set_title('Evolución del error CV en función de la regularización')\n",
        "ax.set_xlabel('alpha')\n",
        "ax.set_ylabel('RMSE')\n",
        "plt.legend();\n"
      ],
      "metadata": {
        "id": "dnZjfsReRyiJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mejor valor alpha encontrado\n",
        "# ==============================================================================\n",
        "print(f\"Mejor valor de alpha encontrado: {regrLasso.alpha_}\")"
      ],
      "metadata": {
        "id": "UpHM3-RoSYp2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Coeficientes del modelo\n",
        "# ==============================================================================\n",
        "df_coeficientes = pd.DataFrame(\n",
        "                        {'predictor': housing.feature_names,\n",
        "                         'coef': regrLasso.coef_.flatten()}\n",
        "                  )\n",
        "fig, ax = plt.subplots(figsize=(11, 3.84))\n",
        "ax.stem(df_coeficientes.predictor, df_coeficientes.coef, markerfmt=' ')\n",
        "plt.xticks(rotation=90, ha='right', size=5)\n",
        "ax.set_xlabel('variable')\n",
        "ax.set_ylabel('coeficientes')\n",
        "ax.set_title('Coeficientes del modelo');"
      ],
      "metadata": {
        "id": "yeTJza4sTLda"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comprobar como sólo se utilizan, básicamente, seis predictores."
      ],
      "metadata": {
        "id": "Q-r6uOGfTrgk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Elastic Net"
      ],
      "metadata": {
        "id": "zhOip6WPT5Jq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# Por defecto ElasticNetCV utiliza el mean squared error\n",
        "regrElastic = ElasticNetCV(\n",
        "            l1_ratio        = [0, 0.1, 0.5, 0.7, 0.9, 0.95, 0.99],\n",
        "            alphas          = np.logspace(-10, 3, 200),\n",
        "            cv              = 10\n",
        "         )\n",
        "np.random.seed(42)\n",
        "_ = regrElastic.fit(X = X_train, y = y_train)\n",
        "print(f\"RMSE de regresión lineal: {rmse(y_test, regrElastic.predict(X=X_test))}\")\n",
        "pintaResultados(regrElastic,50)"
      ],
      "metadata": {
        "id": "11Ia_q_xUAU5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evolución del error en función de alpha y l1_ratio\n",
        "# ==============================================================================\n",
        "# regrElastic.mse_path_ almacena el mse de cv para cada valor de alpha y l1_ratio.\n",
        "# Tiene dimensiones (n_l1_ratio, n_alpha, n_folds)\n",
        "\n",
        "# Error medio de las 10 particiones por cada valor de alpha y l1_ratio \n",
        "mean_error_cv = regrElastic.mse_path_.mean(axis =2)\n",
        "\n",
        "# El resultado es un array de dimensiones (n_l1_ratio, n_alpha)\n",
        "# Se convierte en un dataframe\n",
        "df_resultados_cv = pd.DataFrame(\n",
        "                        data   = mean_error_cv.flatten(),\n",
        "                        index  = pd.MultiIndex.from_product(\n",
        "                                    iterables = [regrElastic.l1_ratio, regrElastic.alphas_],\n",
        "                                    names     = ['l1_ratio', 'modelo.alphas_']\n",
        "                                 ),\n",
        "                        columns = [\"mse_cv\"]\n",
        "                    )\n",
        "\n",
        "df_resultados_cv['rmse_cv'] = np.sqrt(df_resultados_cv['mse_cv'])\n",
        "df_resultados_cv = df_resultados_cv.reset_index().sort_values('mse_cv', ascending = True)\n",
        "df_resultados_cv"
      ],
      "metadata": {
        "id": "Zn473D7jUXXT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mejor valor encontrado para cada l1_ratio\n",
        "fig, ax = plt.subplots(figsize=(7, 3.84))\n",
        "df_resultados_cv.groupby('l1_ratio')['rmse_cv'].min().plot(ax = ax)\n",
        "ax.set_title('Evolución del error CV en función de la l1_ratio')\n",
        "ax.set_xlabel('l1_ratio')\n",
        "ax.set_ylabel('rmse_cv');"
      ],
      "metadata": {
        "id": "5ag5UrIoUsfd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mejor valor alpha y l1_ratio_ encontrado\n",
        "# ==============================================================================\n",
        "print(f\"Mejor valor de alpha encontrado: {regrElastic.alpha_}\")\n",
        "print(f\"Mejor valor de l1_ratio encontrado: {regrElastic.l1_ratio_}\")"
      ],
      "metadata": {
        "id": "bdC7DxOeU34r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Coeficientes del modelo\n",
        "# ==============================================================================\n",
        "df_coeficientes = pd.DataFrame(\n",
        "                        {'predictor': housing.feature_names,\n",
        "                         'coef': regrElastic.coef_.flatten()}\n",
        "                  )\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(11, 3.84))\n",
        "ax.stem(df_coeficientes.predictor, df_coeficientes.coef, markerfmt=' ')\n",
        "plt.xticks(rotation=90, ha='right', size=5)\n",
        "ax.set_xlabel('variable')\n",
        "ax.set_ylabel('coeficientes')\n",
        "ax.set_title('Coeficientes del modelo');\n"
      ],
      "metadata": {
        "id": "MuV7vl6vVCRR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Comparamos"
      ],
      "metadata": {
        "id": "a28AuPepVpXn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rmse_lineal = rmse(y_test, pipe_regr.predict(X=X_test))\n",
        "rmse_ridge = rmse(y_test, regrRidge.predict(X=X_test))\n",
        "rmse_lasso = rmse(y_test, regrLasso.predict(X=X_test))\n",
        "rmse_elastic = rmse(y_test, regrElastic.predict(X=X_test))\n",
        "df_comparacion = pd.DataFrame({\n",
        "                    'modelo': ['Lineal', 'Ridge', 'Lasso', 'Elastic-net'],\n",
        "                    'test rmse': [rmse_lineal, rmse_ridge, rmse_lasso, rmse_elastic]\n",
        "                 })\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(7, 3.84))\n",
        "df_comparacion.set_index('modelo').plot(kind='barh', ax=ax)\n",
        "ax.set_xlabel('rmse')\n",
        "ax.set_ylabel('modelo')\n",
        "ax.set_title('Comparación de modelos');"
      ],
      "metadata": {
        "id": "Za4vsD40VrT5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "En este problema no hay mucha diferencia entre el modelo lineal y los modelos regularizados en cuanto al rmse. Sin embargo, como hemos visto, el número de predictores en los modelos es menor. Vamos a seleccionar aquellos coeficientes con un valor significativo (>0.5)"
      ],
      "metadata": {
        "id": "55YhF5O5Wnpo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Coeficientes de los distintos modelos\n",
        "# ==============================================================================\n",
        "d = [[\"Lineal\", np.count_nonzero(np.abs(reg_lin.coef_.flatten()) > 0.5)],\n",
        "     [\"Ridge\", np.count_nonzero(np.abs(regrRidge.coef_.flatten()) > 0.5)],\n",
        "     [\"Lasso\", np.count_nonzero(np.abs(regrLasso.coef_.flatten()) > 0.5)],\n",
        "     [\"Elastic Net\", np.count_nonzero(np.abs(regrElastic.coef_.flatten()) > 0.5)]]\n",
        "\n",
        "df = pd.DataFrame(d, columns = ['Modelo','Núm. predictores'])\n",
        "print(df)\n"
      ],
      "metadata": {
        "id": "qKCFzzlZAZ3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regresión polinómica\n",
        "Pasemos ahora a intentar una regresión polinómica. Haremos una búsqueda de los mejores hiperparámetros:\n",
        "- 'grades': Grado del polinomio [2,3,4,5,6]"
      ],
      "metadata": {
        "id": "OA7HWqL5kXg2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.model_selection import cross_val_score \n",
        "\n",
        "degrees = [2, 3, 4, 5, 6] # Change degree \"hyperparameter\" here\n",
        "best_score = 0\n",
        "best_degree = 0\n",
        "for degree in degrees:\n",
        "    poly_features = PolynomialFeatures(degree = degree, include_bias=False)\n",
        "    X_train_poly = poly_features.fit_transform(X_train)\n",
        "    polynomial_regressor = LinearRegression()\n",
        "    polynomial_regressor.fit(X_train_poly, y_train)\n",
        "    scores = cross_val_score(polynomial_regressor, X_train_poly, y_train, cv=3) # Change k-fold cv value here\n",
        "    if max(scores) > best_score:\n",
        "      best_score = max(scores)\n",
        "      best_degree = degree     "
      ],
      "metadata": {
        "id": "gUDwQIE3kYjG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Mejor valor (R2): {best_score}\")\n",
        "print(f\"Mejor valor de grado encontrado: {best_degree}\")"
      ],
      "metadata": {
        "id": "M8NDhsyHkkNf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creamos el modelo polinomial con los mejores parámetros:"
      ],
      "metadata": {
        "id": "FuARagPzkmLp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "poly_features = PolynomialFeatures(degree = best_degree, include_bias=False)\n",
        "X_train_poly = poly_features.fit_transform(X_train)\n",
        "best_polynomial_regressor = LinearRegression()\n",
        "best_polynomial_regressor.fit(X_train_poly, y_train)\n",
        "X_test_poly = poly_features.fit_transform(X_test)\n",
        "print(f\"RMSE de regresión polinomial: {rmse(y_test, best_polynomial_regressor.predict(X=X_test_poly))}\")\n",
        "plt.subplots(figsize=(30, 5))\n",
        "x = np.arange(y_test[:50].size)\n",
        "y_pred = best_polynomial_regressor.predict(X=X_test_poly)\n",
        "plt.plot(x, y_test[:50], 'b.', x, y_pred[:50], 'g^')\n"
      ],
      "metadata": {
        "id": "lLpP7hrZko70"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(best_polynomial_regressor.coef_)"
      ],
      "metadata": {
        "id": "hfGfwHiFfOZ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "50D5o3LafUNc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}